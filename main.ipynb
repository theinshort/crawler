{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOk744f7uN6LpIENX+Akj8t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theinshort/crawler/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB DATA CRAWLING\n",
        "\n",
        "In this project our aim is to scrap a webiste using python.\n",
        "\n",
        "`This project is for learning purpose and is not intended to perform any kind of violations.`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJ7DEKQqJugv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "We will use a web crawler to acquire data from a specific domain\n",
        "\n",
        "1. Choose a domains of interest (e.g., news articles, product reviews, scientific publications etc).\n",
        "2. Identify and use web crawling tools or libraries (such as BeautifulSoup, Scrapy, or others) to extract data from the chosen domains.\n",
        "3. Collect a sufficient amount of data to ensure diversity and relevance.\n",
        "4. Scrape and clean the HTML contents to generate clean text outputs (at least 2 GB textual data, the more than better)."
      ],
      "metadata": {
        "id": "3eRKIE2cRhTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Outcome\n",
        "\n",
        "\n",
        "1. Colab Notebook:\n",
        "- Showcases the entire process of web data crawling, including the\n",
        "chosen domains, code implementation, and data extraction.\n",
        "- Clearly comment and document each step in the notebook.\n",
        "2. Dataset Files:\n",
        "- Extracted dataset in a separate file format (e.g., CSV, JSON) that includes a sample of the collected data.\n",
        "3. Summary:\n",
        "- Why specific domains were selected.\n",
        "- Briefly describe the web crawling tools or libraries used and why?\n",
        "- Statistics of data extracted from each domain."
      ],
      "metadata": {
        "id": "AJ9JMvRtReM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHOOSING A DOMAIN\n",
        "In this section we will be performing some steps in order to finalize our domain of interest. We will be considering all ethical an legal concerns before staring our scaping process.\n",
        "\n",
        "In order to choose the domain for scrapping, we have to understand the complexity of data and website structure. We will be focusing toward product based websites like shopping stores because data from these website are usually availble to scrap.\n",
        "\n",
        "Data is useful and scrapping from a website without permission is illegal, so before starting to scrap data we need to check if the data available on the website is allowed for scrapping or not.\n",
        "\n",
        "We will be performing following steps to finalize our domains:\n",
        "\n",
        "\n",
        "1.   Decide what type of data we need to scrap.\n",
        "2.   Find related websites\n",
        "3.   Analyze website content and structure\n",
        "4.   Check website robots.txt file to check restrictions\n",
        "5.   Select website if allowed\n",
        "\n"
      ],
      "metadata": {
        "id": "xPA0-anjN0a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decide what type of data we need to scrap.\n",
        "We will be scraping MCQs data, which is usefull in many aspects and also have some dificulties which will help us understand the scrapping procedure in better way.\n",
        "MCQs data has a structure with multiple options, title, answer, explanations, and more. This type of data is usefull in machine learning and fine-tuning models to get desire results."
      ],
      "metadata": {
        "id": "GHHPPnm4RolM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find related websites\n",
        "Some of the websites with the desired data are as follows:\n",
        "1. [PakMCQs ](https://pakmcqs.com/)\n",
        "2. [CSSMCQs](https://cssmcqs.com/)\n",
        "3. [MCQs Forum](https://mcqsforum.com/)\n",
        "4. [MCQs Planet](https://mcqsplanet.com/)\n",
        "5. [Top MCQs](https://topmcqs.com/)\n"
      ],
      "metadata": {
        "id": "KW7c7uztSu3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze website content and structure\n",
        "\n",
        " After analyzing these websites we have colcluded that the structure of website asre different so we can not use a single method for all website, we have to handle each website indivisually."
      ],
      "metadata": {
        "id": "3xetWck-UoDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check website robots.txt file for restrictions\n",
        "\n",
        "In order to scrap a website we have to first check if the website is allowing developers and other users to scrap their content. To check restricions, we need to analyse the website's robots.txt file.\n"
      ],
      "metadata": {
        "id": "Z-QWK9Stb_La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import requests as req\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Creating a function that fetch the content of a robots.txt file\n",
        "def get_robots_txt(url):\n",
        "  \"\"\"Fetch robots.txt file content from given url \"\"\"\n",
        "  file_url = f\"{url}/robots.txt\"\n",
        "  # Fetching data from file using request library get function\n",
        "  response = req.get(file_url)\n",
        "  # Check the status of response before return\n",
        "  if response.status_code == 200:\n",
        "    return response.text\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def check_restrictions(url, robots_txt):\n",
        "  \"\"\"Check the robots.txt rules to check if URL is allowed for scrapping or not \"\"\"\n",
        "\n",
        "  if not robots_txt:\n",
        "    return True\n",
        "\n",
        "  soup = BeautifulSoup(robots_txt, \"html.parser\")\n",
        "  # Adding a user agent header help mimic a real browser and reduce the chances of getting blocked.\n",
        "  user_agents = soup.find_all(\"user-agent\")\n",
        "\n",
        "  for user_agent in user_agents:\n",
        "    # Checks the wildcard for user agent in rorbots.txt file content\n",
        "    if \"*\" in user_agent.text.strip():\n",
        "      for disallow in soup.find_all(\"disallow\"):\n",
        "        disallow_path = disallow.text.strip()\n",
        "        # Check if the url is in the restricted paths or not\n",
        "        if disallow_path in url:\n",
        "          return False\n",
        "  # There are nor restricted rules available in the file. We will consider it as allowed for scrapping\n",
        "  return True\n",
        "\n"
      ],
      "metadata": {
        "id": "zU4EUYU7eoc4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we have done in the above code example is:\n",
        "1. Fetches the requests library for making web requests and BeautifulSoup for parsing HTML content.\n",
        "2. Retrieves the `robots.txt` file from a given URL using `get_robots_txt()` function.\n",
        "3. Analyzes the `robots.txt` content within `check_restrictions()` to determine if a URL is allowed for scraping based on website guidelines before proceeding."
      ],
      "metadata": {
        "id": "L2eRMzPOteBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "site_url = \"https://pakmcqs.com/\"\n",
        "robots_txt = get_robots_txt(site_url)\n",
        "\n",
        "if robots_txt:\n",
        "  # set target url to check for crawling\n",
        "  target_url = \"https://pakmcqs.com/category/english-mcqs\"\n",
        "  allowed = check_restrictions(target_url, robots_txt)\n",
        "  if allowed:\n",
        "    print(f\"Target URL: '{target_url}' is allowed for Crawling\")\n",
        "  else:\n",
        "    print(f\"Target URL: '{target_url}' is not allowed for crawling\")\n",
        "\n",
        "else:\n",
        "  print(\"robots.txt not found, Ask for confirmation of assume this as no restricions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pycl31CQuzqu",
        "outputId": "255386ec-3e9d-47b8-81e8-51dd1019e245"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target URL: 'https://pakmcqs.com/category/english-mcqs' is allowed for Crawling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-06d763deb786>:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(robots_txt, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After verification, we can confirm that the target url is allowed for scraping and we can proceed to the next step."
      ],
      "metadata": {
        "id": "NZJv-T0zxHg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_category_urls(url):\n",
        "  \"\"\"\n",
        "  Fetches HTML from a given URL, extracts URLs with a specific prefix from the div with id=\"secondary\".\n",
        "\n",
        "  Args:\n",
        "      url (str): The URL to fetch HTML from.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of URLs starting with \"https://example.com/category/\" found within the div.\n",
        "  \"\"\"\n",
        "\n",
        "  # Send a GET request to the provided URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check for successful response\n",
        "  if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the element with id=\"secondary\"\n",
        "    secondary_div = soup.find('div', id='secondary')\n",
        "\n",
        "    # Create an empty list to store the desired URLs\n",
        "    category_urls = []\n",
        "\n",
        "    # Check if the div is found\n",
        "    if secondary_div:\n",
        "      # Find all anchor tags (a elements) within the div\n",
        "      for anchor in secondary_div.find_all('a'):\n",
        "        href = anchor.get('href')\n",
        "        if href and href.startswith(\"https://pakmcqs.com/category/\"):\n",
        "          category_urls.append(href)\n",
        "\n",
        "    return category_urls\n",
        "  else:\n",
        "    print(f\"Error fetching URL: {url} - Status code: {response.status_code}\")\n",
        "    return []\n",
        "\n",
        "# Example usage\n",
        "url = \"https://pakmcqs.com/\"  # Replace with the actual website URL\n",
        "category_urls = extract_category_urls(url)\n",
        "\n",
        "if category_urls:\n",
        "  print(f\"Extracted category URLs: {category_urls}\")\n",
        "\n",
        "else:\n",
        "  print(\"No category URLs found in the provided URL or error fetching the content.\")\n"
      ],
      "metadata": {
        "id": "GU0pyaPVDTt5",
        "outputId": "c4bca293-86df-4d65-f988-4d227e426e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted category URLs: ['https://pakmcqs.com/category/english-mcqs', 'https://pakmcqs.com/category/mathematics-mcqs', 'https://pakmcqs.com/category/general_knowledge_mcqs', 'https://pakmcqs.com/category/pakistan-current-affairs-mcqs', 'https://pakmcqs.com/category/world-current-affairs-mcqs', 'https://pakmcqs.com/category/pak-study-mcqs', 'https://pakmcqs.com/category/islamic-studies-mcqs', 'https://pakmcqs.com/category/computer-mcqs', 'https://pakmcqs.com/category/everyday-science-mcqs', 'https://pakmcqs.com/category/physics-mcqs', 'https://pakmcqs.com/category/chemistry-mcqs', 'https://pakmcqs.com/category/biology-mcqs', 'https://pakmcqs.com/category/pedagogy-mcqs', 'https://pakmcqs.com/category/urdu-general-knowledge', 'https://pakmcqs.com/category/finance-mcqs', 'https://pakmcqs.com/category/hrm-mcqs', 'https://pakmcqs.com/category/marketing-mcqs', 'https://pakmcqs.com/category/accounting-mcqs', 'https://pakmcqs.com/category/auditing-mcqs', 'https://pakmcqs.com/category/electrical-engineering-mcqs', 'https://pakmcqs.com/category/civil-engineering-mcqs', 'https://pakmcqs.com/category/mechanical-engineering-mcqs', 'https://pakmcqs.com/category/chemical-engineering', 'https://pakmcqs.com/category/software-engineering-mcqs', 'https://pakmcqs.com/category/medical-mcqs', 'https://pakmcqs.com/category/medical-mcqs/biochemistry', 'https://pakmcqs.com/category/medical-mcqs/dental-materials', 'https://pakmcqs.com/category/medical-mcqs/general-anatomy-mcqs', 'https://pakmcqs.com/category/medical-mcqs/microbiology', 'https://pakmcqs.com/category/medical-mcqs/oral-anatomy', 'https://pakmcqs.com/category/medical-mcqs/oral-histology', 'https://pakmcqs.com/category/medical-mcqs/oral-pathology-and-medicine', 'https://pakmcqs.com/category/medical-mcqs/physiology-mcqs', 'https://pakmcqs.com/category/medical-mcqs/pathology', 'https://pakmcqs.com/category/medical-mcqs/pharmacology', 'https://pakmcqs.com/category/psychology-mcqs', 'https://pakmcqs.com/category/agriculture-mcqs', 'https://pakmcqs.com/category/economics-mcqs', 'https://pakmcqs.com/category/sociology-mcqs', 'https://pakmcqs.com/category/political-science-mcqs', 'https://pakmcqs.com/category/statistics-mcqs', 'https://pakmcqs.com/category/english-literature-mcqs', 'https://pakmcqs.com/category/judiciary-and-law-mcqs', 'https://pakmcqs.com/category/election-officer-mcqs']\n"
          ]
        }
      ]
    }
  ]
}