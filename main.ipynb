{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMKlqe8lDXD4TyQbZ1aTWVi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theinshort/crawler/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB DATA CRAWLING\n",
        "\n",
        "In this project our aim is to scrap a webiste using python.\n",
        "\n",
        "`This project is for learning purpose and is not intended to perform any kind of violations.`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJ7DEKQqJugv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "We will use a web crawler to acquire data from a specific domain\n",
        "\n",
        "1. Choose a domains of interest (e.g., news articles, product reviews, scientific publications etc).\n",
        "2. Identify and use web crawling tools or libraries (such as BeautifulSoup, Scrapy, or others) to extract data from the chosen domains.\n",
        "3. Collect a sufficient amount of data to ensure diversity and relevance.\n",
        "4. Scrape and clean the HTML contents to generate clean text outputs (at least 2 GB textual data, the more than better)."
      ],
      "metadata": {
        "id": "3eRKIE2cRhTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Outcome\n",
        "\n",
        "\n",
        "1. Colab Notebook:\n",
        "- Showcases the entire process of web data crawling, including the\n",
        "chosen domains, code implementation, and data extraction.\n",
        "- Clearly comment and document each step in the notebook.\n",
        "2. Dataset Files:\n",
        "- Extracted dataset in a separate file format (e.g., CSV, JSON) that includes a sample of the collected data.\n",
        "3. Summary:\n",
        "- Why specific domains were selected.\n",
        "- Briefly describe the web crawling tools or libraries used and why?\n",
        "- Statistics of data extracted from each domain."
      ],
      "metadata": {
        "id": "AJ9JMvRtReM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing A Domain\n",
        "In this section we will be performing some steps in order to finalize our domain of interest. We will be considering all ethical an legal concerns before staring our scaping process.\n",
        "\n",
        "In order to choose the domain for scrapping, we have to understand the complexity of data and website structure. We will be focusing toward product based websites like shopping stores because data from these website are usually availble to scrap.\n",
        "\n",
        "Data is useful and scrapping from a website without permission is illegal, so before starting to scrap data we need to check if the data available on the website is allowed for scrapping or not.\n",
        "\n",
        "We will be performing following steps to finalize our domains:\n",
        "\n",
        "\n",
        "1.   Decide what type of data we need to scrap.\n",
        "2.   Find related websites\n",
        "3.   Analyze website content and structure\n",
        "4.   Check website robots.txt file to check restrictions\n",
        "5.   Select website if allowed\n",
        "\n"
      ],
      "metadata": {
        "id": "xPA0-anjN0a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decide what type of data we need to scrap.\n",
        "We will be scraping MCQs data, which is usefull in many aspects and also have some dificulties which will help us understand the scrapping procedure in better way.\n",
        "MCQs data has a structure with multiple options, title, answer, explanations, and more. This type of data is usefull in machine learning and fine-tuning models to get desire results."
      ],
      "metadata": {
        "id": "GHHPPnm4RolM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find related websites\n",
        "Some of the websites with the desired data are as follows:\n",
        "1. [PakMCQs ](https://pakmcqs.com/)\n",
        "2. [CSSMCQs](https://cssmcqs.com/)\n",
        "3. [MCQs Forum](https://mcqsforum.com/)\n",
        "4. [MCQs Planet](https://mcqsplanet.com/)\n",
        "5. [Top MCQs](https://topmcqs.com/)\n"
      ],
      "metadata": {
        "id": "KW7c7uztSu3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze website content and structure\n",
        "\n",
        " After analyzing these websites we have colcluded that the structure of website asre different so we can not use a single method for all website, we have to handle each website indivisually."
      ],
      "metadata": {
        "id": "3xetWck-UoDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check website robots.txt file for restrictions\n",
        "\n",
        "In order to scrap a website we have to first check if the website is allowing developers and other users to scrap their content. To check restricions, we need to analyse the website's robots.txt file.\n"
      ],
      "metadata": {
        "id": "Z-QWK9Stb_La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import requests as req\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Creating a function that fetch the content of a robots.txt file\n",
        "def get_robots_txt(url):\n",
        "  \"\"\"Fetch robots.txt file content from given url \"\"\"\n",
        "  file_url = f\"{url}/robots.txt\"\n",
        "  # Fetching data from file using request library get function\n",
        "  response = req.get(file_url)\n",
        "  # Check the status of response before return\n",
        "  if response.status_code == 200:\n",
        "    return response.text\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def check_restrictions(url, robots_txt):\n",
        "  \"\"\"Check the robots.txt rules to check if URL is allowed for scrapping or not \"\"\"\n",
        "\n",
        "  if not robots_txt:\n",
        "    return True\n",
        "\n",
        "  soup = BeautifulSoup(robots_txt, \"html.parser\")\n",
        "  # Adding a user agent header help mimic a real browser and reduce the chances of getting blocked.\n",
        "  user_agents = soup.find_all(\"user-agent\")\n",
        "\n",
        "  for user_agent in user_agents:\n",
        "    # Checks the wildcard for user agent in rorbots.txt file content\n",
        "    if \"*\" in user_agent.text.strip():\n",
        "      for disallow in soup.find_all(\"disallow\"):\n",
        "        disallow_path = disallow.text.strip()\n",
        "        # Check if the url is in the restricted paths or not\n",
        "        if disallow_path in url:\n",
        "          return False\n",
        "  # There are nor restricted rules available in the file. We will consider it as allowed for scrapping\n",
        "  return True\n",
        "\n"
      ],
      "metadata": {
        "id": "zU4EUYU7eoc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we have done in the above code example is:\n",
        "1. Fetches the requests library for making web requests and BeautifulSoup for parsing HTML content.\n",
        "2. Retrieves the `robots.txt` file from a given URL using `get_robots_txt()` function.\n",
        "3. Analyzes the `robots.txt` content within `check_restrictions()` to determine if a URL is allowed for scraping based on website guidelines before proceeding."
      ],
      "metadata": {
        "id": "L2eRMzPOteBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "site_url = \"https://pakmcqs.com/\"\n",
        "robots_txt = get_robots_txt(site_url)\n",
        "\n",
        "if robots_txt:\n",
        "  # set target url to check for crawling\n",
        "  target_url = \"https://pakmcqs.com/category/english-mcqs\"\n",
        "  allowed = check_restrictions(target_url, robots_txt)\n",
        "  if allowed:\n",
        "    print(f\"Target URL: '{target_url}' is allowed for Crawling\")\n",
        "  else:\n",
        "    print(f\"Target URL: '{target_url}' is not allowed for Crawling\")\n",
        "\n",
        "else:\n",
        "  print(\"robots.txt not found, Ask for confirmation of assume this as no restricions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pycl31CQuzqu",
        "outputId": "99b5ffa3-8ebe-4b59-ade3-3f7c2073dc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target URL: 'https://pakmcqs.com/category/english-mcqs' is allowed for Crawling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-06d763deb786>:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(robots_txt, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After verification, we can confirm that the target url is allowed for scraping and we can proceed to the next step."
      ],
      "metadata": {
        "id": "NZJv-T0zxHg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Target URLs For Scrapping\n",
        "Now we nee to to fetch the urls before statring to scrap the content. This will help us easily create a function to scrap the required data.\n",
        "Foo this, we have to get the HTML tag whhich contains the categories urls. Then we create a function that get all the urls inside that html tag using `href`."
      ],
      "metadata": {
        "id": "PUQW9dmuCJ_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_target_urls(base_url, starts_with, html_tag, tag_id):\n",
        "  \"\"\"\n",
        "  Fetch list of target urls from the given base url and match urls string provided with html tag and tag id\n",
        "\n",
        "  Args:\n",
        "      base_url (str): The URL to fetch HTML from.\n",
        "      starts_with (str):The starting URL to compare with the URLs to fetch.\n",
        "      html_tag (str): HTML Tag from which urls needs to be fetched.\n",
        "      tag_id (str): Tag id to fetch target URLs from.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of URLs starting with the starts_with Args found within the html_tag with given tag_id.\n",
        "  \"\"\"\n",
        "\n",
        "  # Send a GET request to the provided URL\n",
        "  user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"\n",
        "\n",
        "  headers = {\n",
        "    'User-Agent': user_agent,\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.5'\n",
        "  }\n",
        "  response = requests.get(base_url, headers=headers)\n",
        "\n",
        "\n",
        "  # Checkif the URL is active\n",
        "  if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the element with provided html_tag & tag_id\n",
        "    tag_content = soup.find(html_tag, id=tag_id)\n",
        "\n",
        "    # Create an empty list to store the target URLs\n",
        "    target_urls = []\n",
        "\n",
        "    # Check if the div is found\n",
        "    if tag_content:\n",
        "      # Find all anchor tags (a elements) within the div\n",
        "      for anchor in tag_content.find_all('a'):\n",
        "        href = anchor.get('href')\n",
        "        if href and href.startswith(starts_with):\n",
        "          target_urls.append(href)\n",
        "\n",
        "    return target_urls\n",
        "  else:\n",
        "    print(f\"Error fetching URL: {base_url} - Status code: {response.status_code}\")\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "GU0pyaPVDTt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching Category Name From URL\n",
        "In order to increade our dataset quality, we have to add a saperate column for catregory. we will use the trick to extract the last element from url which depict the category of MCQs. Which we will use in our mcqs dictionary."
      ],
      "metadata": {
        "id": "tElUljF0DuFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_category_from_url(url):\n",
        "  \"\"\"Extract Category from provided url using split function\"\"\"\n",
        "  url_parts = url.split(\"/\")\n",
        "  return url_parts[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "Srgh235CwsXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing Code To Extract Data\n",
        "In this section, we will create a function that extract all MCQs from a single page, and extract the MCQs components like Question, Options, Answers and Category. Then we will arrange them in a dictionary, creating a list of MCQ and then return."
      ],
      "metadata": {
        "id": "zueRCdCGDzWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_mcq(url, category):\n",
        "  \"\"\"\n",
        "  Extracts MCQ details (question, answer choices) from a given URL.\n",
        "  Args:\n",
        "      url (str): The URL of the MCQ page.\n",
        "  Returns:\n",
        "      dict: A dictionary containing the extracted MCQ details (question, answers).\n",
        "  \"\"\"\n",
        "  all_mcqs = []\n",
        "  user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"\n",
        "\n",
        "  headers = {\n",
        "    'User-Agent': user_agent,\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.5'\n",
        "  }\n",
        "  # Adding headers and user agent to pretent to be a user.\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the header containing the MCQ question\n",
        "    headers = soup.find_all('header', class_='entry-header entry-header-index')\n",
        "    for header in headers:\n",
        "      mcq = {}\n",
        "      if header:\n",
        "        mcq[\"category\"] = category\n",
        "        # Extract question from the anchor tag within the header\n",
        "        question_element = header.find('strong')\n",
        "        if question_element:\n",
        "          mcq['question'] = question_element.text.strip()\n",
        "\n",
        "        # Find the answer choices within the content section\n",
        "        content = header.find('div', class_='entry-content')\n",
        "        if content:\n",
        "          option_elements = content.find_all('p')\n",
        "          # correct_answer = content.find('strong')\n",
        "          if option_elements:\n",
        "            # Assuming the first paragraph contains answer choices (modify if needed)\n",
        "            options = option_elements[0].text.strip().split('\\n')\n",
        "            options_list = [option.strip() for option in options]\n",
        "            if len(options_list) > 6:\n",
        "              break\n",
        "            for i, option in enumerate(options_list):\n",
        "              mcq[f\"option {i+1}\"] = re.sub(r\"(A\\. |B\\. |C\\. |D\\. |E\\. |F\\. )\", \"\", option)\n",
        "\n",
        "          # Identify bold answer by searching for strong tags within paragraphs\n",
        "          bold_answer = \"\"\n",
        "          for option_element in option_elements:\n",
        "            strong_element = option_element.find('strong')\n",
        "            if strong_element:\n",
        "              bold_answer = strong_element.text.strip()\n",
        "              break  # Exit after finding the first bold element (assuming only one)\n",
        "          if bold_answer:\n",
        "            mcq['correct_answer'] = bold_answer.strip()[0]\n",
        "            all_mcqs.append(mcq)\n",
        "\n",
        "  return all_mcqs\n"
      ],
      "metadata": {
        "id": "pOtYexCKzVnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Max Page Numbers\n",
        "Each url have multiple pages, Each Page contain equal number of MCQs. We have to get the exact number of pages to iterate the loop through each page to get the data."
      ],
      "metadata": {
        "id": "B2f3GZurNttf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_page_number(url):\n",
        "  \"\"\"Get the max number of pages a single url have\"\"\"\n",
        "  max_page = 1000\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    page_nav = soup.find(\"div\", class_=\"wpsp-page-nav\")\n",
        "    page_numbers = page_nav.find_all(\"a\")\n",
        "    max_page = int(page_numbers[-2].text)\n",
        "\n",
        "  return max_page\n"
      ],
      "metadata": {
        "id": "eooImPuyd0iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get All MCQs For Single URL\n",
        "In this function, we will iterate through the max number of the pages, and fetch MCQs from each page. Then axtend each fetched mcqs list to all_mcqs list and return."
      ],
      "metadata": {
        "id": "NL0u3ME7Oocp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_page_mcqs(url, last_page):\n",
        "  all_mcqs = []\n",
        "  category = get_category_from_url(url)\n",
        "  for x in range(1,last_page+1):\n",
        "    mcqs = extract_mcq(url+f\"/page/{x}\", category)\n",
        "    all_mcqs.extend(mcqs)\n",
        "\n",
        "  write_mcqs_to_csv(all_mcqs, \"mcqs.csv\")\n"
      ],
      "metadata": {
        "id": "-IV2FCekcc4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Threading To Speed Up\n",
        "For each URL, we will be creating a saperate thread. This will improve the performance and speed by handling each url in a seperate thread."
      ],
      "metadata": {
        "id": "Xlt2Ffm9O07H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading as td\n",
        "\n",
        "def fetch_all_mcqs(urls):\n",
        "  threads = []\n",
        "  # Function that work as target for thread\n",
        "  def process_url(url):\n",
        "    max_page = get_max_page_number(url) # To get the max number of pages a single url has\n",
        "    get_all_page_mcqs(url, max_page) # Fetch all page mcqs\n",
        "\n",
        "  for url in urls:\n",
        "    # Creating threads for each url\n",
        "    thread = td.Thread(target=process_url, args=(url,))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "  for thread in threads:\n",
        "    thread.join()\n",
        "\n"
      ],
      "metadata": {
        "id": "7HjLHtrTbWPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing Data To CSV\n",
        "After getting MCQs from each url, we will be adding MCQs to a csv file."
      ],
      "metadata": {
        "id": "Urw2hSI6PJ8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def write_mcqs_to_csv(all_mcqs, file_name):\n",
        "  with open(file_name, \"a\", newline='') as csv_file:\n",
        "    # Creates a header for the csv file\n",
        "    columns = [\"question\", \"option 1\", \"option 2\", \"option 3\", \"option 4\",\"option 5\", \"option 6\",  \"correct_answer\", \"category\"]\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=columns)\n",
        "    writer.writeheader()\n",
        "    # Writer write all mcqs at once\n",
        "    writer.writerows(all_mcqs)"
      ],
      "metadata": {
        "id": "VLmKU08riyDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching Target URLs of single Domain\n",
        "Here, we will fetching all the urls from the given html tag. This will then used to fetch all the mcqs saperately."
      ],
      "metadata": {
        "id": "KW73lreMPWVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Website url to fecth the categories urls list\n",
        "base_url = \"https://pakmcqs.com/\"\n",
        "\n",
        "# this function takes a base url, starting url matching string, the html tag and class name\n",
        "target_urls = get_target_urls(base_url,'https://pakmcqs.com/category/', 'div', 'secondary')\n",
        "\n",
        "if target_urls:\n",
        "  print(f\"Extracted Category URLs Successfully\")\n",
        "else:\n",
        "  print(\"No category URLs found in the provided URL or error fetching the content.\")\n"
      ],
      "metadata": {
        "id": "DdlEMP0BCYbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching All MCQs\n",
        "We will use those urls to fetch all mcqs and add them in a csv file"
      ],
      "metadata": {
        "id": "MLxASQnXPl5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch All MCQs and write To CSV files\n",
        "fetch_all_mcqs(target_urls)"
      ],
      "metadata": {
        "id": "ai165NPGbUkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Data\n",
        "Let us check what data we have scrapped so far and get some insights."
      ],
      "metadata": {
        "id": "lGHId3_BFLWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def analyze_mcqs(csv_file):\n",
        "  \"\"\"\n",
        "  Analyzes MCQs data from a CSV file and provides comprehensive statistics.\n",
        "\n",
        "  Args:\n",
        "      csv_file (str): The path to the CSV file containing MCQ data.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary containing detailed statistics about the MCQ data.\n",
        "  \"\"\"\n",
        "\n",
        "  num_mcqs = 0\n",
        "  category_counts = {}\n",
        "  answer_lengths = []\n",
        "  # Track null values in each column\n",
        "  null_values_per_column = {}\n",
        "\n",
        "  try:\n",
        "    with open(csv_file, 'r', newline='') as csvfile:\n",
        "      reader = csv.DictReader(csvfile)\n",
        "      headers = reader.fieldnames\n",
        "\n",
        "      for row in reader:\n",
        "        # Count number of mcqs\n",
        "        num_mcqs += 1\n",
        "        # Count Category for mcqs\n",
        "        category = row['category']\n",
        "        category_counts[category] = category_counts.get(category, 0) + 1\n",
        "\n",
        "        # Extract and analyze answer lengths\n",
        "        for option in range(1, 7):  # Assuming options are in columns 1-6\n",
        "          option_key = f\"option {option}\"\n",
        "          if option_key in row:\n",
        "            answer_lengths.append(len(row[option_key]))\n",
        "\n",
        "        # Check for null values in each column\n",
        "        for header in headers:\n",
        "          null_values_per_column[header] = null_values_per_column.get(header, 0)\n",
        "          if row[header] == '':\n",
        "            null_values_per_column[header] += 1\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: CSV file '{csv_file}' not found.\")\n",
        "    return {}\n",
        "\n",
        "  # Calculate statistics for answer lengths\n",
        "  min_length = min(answer_lengths) if answer_lengths else 0\n",
        "  max_length = max(answer_lengths) if answer_lengths else 0\n",
        "  avg_length = sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0\n",
        "\n",
        "  # Return statistics dictionary\n",
        "  return {\n",
        "      \"number_of_mcqs\": num_mcqs,\n",
        "      \"category_counts\": category_counts,\n",
        "      \"answer_length_stats\": {\n",
        "          \"min_length\": min_length,\n",
        "          \"max_length\": max_length,\n",
        "          \"avg_length\": avg_length\n",
        "      },\n",
        "      \"null_values_per_column\": null_values_per_column\n",
        "  }\n"
      ],
      "metadata": {
        "id": "by1CLcgyFjHF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statistics = analyze_mcqs('mcqs.csv')\n",
        "\n",
        "if statistics:\n",
        "  # Print informative statistics of our scrapped data\n",
        "  print(f\"Number of MCQs: {statistics['number_of_mcqs']}\")\n",
        "\n",
        "  print(\"\\nCategory Distribution:\")\n",
        "  for category, count in statistics['category_counts'].items():\n",
        "    print(f\"- {category}: {count}\")\n",
        "\n",
        "  print(\"\\nAnswer Length Statistics:\")\n",
        "  print(f\"- Minimum answer length: {statistics['answer_length_stats']['min_length']}\")\n",
        "  print(f\"- Maximum answer length: {statistics['answer_length_stats']['max_length']}\")\n",
        "  print(f\"- Average answer length: {statistics['answer_length_stats']['avg_length']:.2f}\")\n",
        "\n",
        "  print(\"\\nNull Values per Column:\")\n",
        "  for column, count in statistics['null_values_per_column'].items():\n",
        "    print(f\"- {column}: {count}\")\n",
        "else:\n",
        "  print(\"No statistics available. Please check if 'mcqs.csv' exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv3n1THPIHUZ",
        "outputId": "0114a41b-a1af-4805-c1ec-81ef74eb614d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of MCQs: 87416\n",
            "\n",
            "Category Distribution:\n",
            "- auditing-mcqs: 153\n",
            "- category: 43\n",
            "- election-officer-mcqs: 167\n",
            "- oral-anatomy: 247\n",
            "- pathology: 333\n",
            "- pedagogy-mcqs: 373\n",
            "- oral-histology: 402\n",
            "- biochemistry: 448\n",
            "- microbiology: 502\n",
            "- urdu-general-knowledge: 445\n",
            "- physiology-mcqs: 522\n",
            "- general-anatomy-mcqs: 538\n",
            "- pharmacology: 576\n",
            "- dental-materials: 627\n",
            "- statistics-mcqs: 737\n",
            "- physics-mcqs: 867\n",
            "- oral-pathology-and-medicine: 953\n",
            "- hrm-mcqs: 1015\n",
            "- finance-mcqs: 1050\n",
            "- computer-mcqs: 1311\n",
            "- chemistry-mcqs: 1371\n",
            "- accounting-mcqs: 1457\n",
            "- islamic-studies-mcqs: 1458\n",
            "- biology-mcqs: 1512\n",
            "- marketing-mcqs: 1730\n",
            "- sociology-mcqs: 1620\n",
            "- everyday-science-mcqs: 1997\n",
            "- pak-study-mcqs: 2479\n",
            "- english-mcqs: 2798\n",
            "- mathematics-mcqs: 1645\n",
            "- psychology-mcqs: 1855\n",
            "- software-engineering-mcqs: 2041\n",
            "- mechanical-engineering-mcqs: 2137\n",
            "- agriculture-mcqs: 2828\n",
            "- judiciary-and-law-mcqs: 2953\n",
            "- general_knowledge_mcqs: 5453\n",
            "- civil-engineering-mcqs: 3052\n",
            "- pakistan-current-affairs-mcqs: 4016\n",
            "- english-literature-mcqs: 3731\n",
            "- electrical-engineering-mcqs: 3700\n",
            "- world-current-affairs-mcqs: 3951\n",
            "- political-science-mcqs: 4158\n",
            "- economics-mcqs: 4566\n",
            "- medical-mcqs: 5136\n",
            "- chemical-engineering: 8463\n",
            "\n",
            "Answer Length Statistics:\n",
            "- Minimum answer length: 0\n",
            "- Maximum answer length: 332\n",
            "- Average answer length: 12.14\n",
            "\n",
            "Null Values per Column:\n",
            "- question: 1\n",
            "- option 1: 317\n",
            "- option 2: 371\n",
            "- option 3: 386\n",
            "- option 4: 539\n",
            "- option 5: 80766\n",
            "- option 6: 87142\n",
            "- option 7: 87373\n",
            "- correct_answer: 0\n",
            "- category: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "We have successfully fetched the MCQs data from a single website. The data is not huge in size, but its actully large in number. MCQs data is not readily availble in huge amount so it is difficult to find such websites that can contain that much of data."
      ],
      "metadata": {
        "id": "l3OyfsYsIiVd"
      }
    }
  ]
}