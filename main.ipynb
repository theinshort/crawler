{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMelde2y7NP2c8BQBritPV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theinshort/crawler/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB DATA CRAWLING\n",
        "\n",
        "In this project our aim is to scrap a webiste using python.\n",
        "\n",
        "`This project is for learning purpose and is not intended to perform any kind of violations.`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJ7DEKQqJugv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "We will use a web crawler to acquire data from a specific domain\n",
        "\n",
        "1. Choose a domains of interest (e.g., news articles, product reviews, scientific publications etc).\n",
        "2. Identify and use web crawling tools or libraries (such as BeautifulSoup, Scrapy, or others) to extract data from the chosen domains.\n",
        "3. Collect a sufficient amount of data to ensure diversity and relevance.\n",
        "4. Scrape and clean the HTML contents to generate clean text outputs (at least 2 GB textual data, the more than better)."
      ],
      "metadata": {
        "id": "3eRKIE2cRhTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Outcome\n",
        "\n",
        "\n",
        "1. Colab Notebook:\n",
        "- Showcases the entire process of web data crawling, including the\n",
        "chosen domains, code implementation, and data extraction.\n",
        "- Clearly comment and document each step in the notebook.\n",
        "2. Dataset Files:\n",
        "- Extracted dataset in a separate file format (e.g., CSV, JSON) that includes a sample of the collected data.\n",
        "3. Summary:\n",
        "- Why specific domains were selected.\n",
        "- Briefly describe the web crawling tools or libraries used and why?\n",
        "- Statistics of data extracted from each domain."
      ],
      "metadata": {
        "id": "AJ9JMvRtReM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHOOSING A DOMAIN\n",
        "In this section we will be performing some steps in order to finalize our domain of interest. We will be considering all ethical an legal concerns before staring our scaping process.\n",
        "\n",
        "In order to choose the domain for scrapping, we have to understand the complexity of data and website structure. We will be focusing toward product based websites like shopping stores because data from these website are usually availble to scrap.\n",
        "\n",
        "Data is useful and scrapping from a website without permission is illegal, so before starting to scrap data we need to check if the data available on the website is allowed for scrapping or not.\n",
        "\n",
        "We will be performing following steps to finalize our domains:\n",
        "\n",
        "\n",
        "1.   Decide what type of data we need to scrap.\n",
        "2.   Find related websites\n",
        "3.   Analyze website content and structure\n",
        "4.   Check website robots.txt file to check restrictions\n",
        "5.   Select website if allowed\n",
        "\n"
      ],
      "metadata": {
        "id": "xPA0-anjN0a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decide what type of data we need to scrap.\n",
        "We will be scraping products data, which is usefull in many aspects and also have some dificulties which will help us understand the scrapping procedure in better way.\n",
        "Products data has a structure with multiple features, categories, sub-categories, price, and more. This type of data is usefull in machine learning and fine-tuning models to get desire results."
      ],
      "metadata": {
        "id": "GHHPPnm4RolM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find related websites\n",
        "Some of the websites with the desired data are as follows:\n",
        "1. [Daraz.pk Online Shopping](https://www.daraz.pk/)\n",
        "2. [Home Shopping Services](https://homeshopping.pk/)\n",
        "3. [Telemart Online Shopping](https://www.telemart.pk/)\n"
      ],
      "metadata": {
        "id": "KW7c7uztSu3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze website content and structure\n",
        "\n",
        " After analyzing these websites we have cam for a colclusion that daraz.pk is a better option for the desired job. As it has a huge database and somewhat average structure, which will be useful to learn scraping. Other websites are not that good for scrapping and the website has limited data."
      ],
      "metadata": {
        "id": "3xetWck-UoDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check website robots.txt file to check restrictions\n",
        "\n",
        "In order to scrap a website we have to first check if the website is allowing developers and other users to scrap their content. To check restricions, we need to analyse the website's robots.txt file.\n"
      ],
      "metadata": {
        "id": "Z-QWK9Stb_La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import requests as req\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Creating a function that fetch the content of a robots.txt file\n",
        "def get_robots_txt(url):\n",
        "  \"\"\"Fetch robots.txt file content from given url \"\"\"\n",
        "  file_url = f\"{url}/robots.txt\"\n",
        "  # Fetching data from file using request library get function\n",
        "  response = req.get(file_url)\n",
        "  # Check the status of response before return\n",
        "  if response.status_code == 200:\n",
        "    return response.txt\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def check_restrictions(url, robots_txt):\n",
        "  \"\"\"Check the robots.txt rules to check if URL is allowed for scrapping or not \"\"\"\n",
        "\n",
        "  if not robots_txt:\n",
        "    return True\n",
        "\n",
        "  soup = BeautifulSoup(robots_txt, \"html.parser\")\n",
        "  # Adding a user agent header help mimic a real browser and reduce the chances of getting blocked.\n",
        "  user_agents = soup.find_all(\"user-agent\")\n",
        "\n",
        "  for user_agent in user_agents:\n",
        "    # Checks the wildcard for user agent in rorbots.txt file content\n",
        "    if \"*\" in user_agent.text.strip():\n",
        "      for disallow in soup.find_all(\"disallow\"):\n",
        "        disallow_path = disallow.text.strip()\n",
        "        # Check if the url is in the restricted paths or not\n",
        "        if disallow_path in url:\n",
        "          return False\n",
        "  # There are nor restricted rules available in the file. We will consider it as allowed for scrapping\n",
        "  return True\n",
        "\n"
      ],
      "metadata": {
        "id": "zU4EUYU7eoc4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we have done in the above code example is:\n",
        "1. Fetches the requests library for making web requests and BeautifulSoup for parsing HTML content.\n",
        "2. Retrieves the `robots.txt` file from a given URL using `get_robots_txt()` function.\n",
        "3. Analyzes the `robots.txt` content within `check_restrictions()` to determine if a URL is allowed for scraping based on website guidelines before proceeding."
      ],
      "metadata": {
        "id": "L2eRMzPOteBP"
      }
    }
  ]
}