{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqWtJIGtDrY1BUI2qNFqaI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theinshort/crawler/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB DATA CRAWLING\n",
        "\n",
        "In this project our aim is to scrap a webiste using python.\n",
        "\n",
        "`This project is for learning purpose and is not intended to perform any kind of violations.`\n",
        "\n",
        "## Objective\n",
        "We will use a web crawler to acquire data from a specific domain\n",
        "\n",
        "1. Choose a domains of interest (e.g., news articles, product reviews, scientific publications etc).\n",
        "2. Identify and use web crawling tools or libraries (such as BeautifulSoup, Scrapy, or others) to extract data from the chosen domains.\n",
        "3. Collect a sufficient amount of data to ensure diversity and relevance.\n",
        "4. Scrape and clean the HTML contents to generate clean text outputs (at least 2 GB textual data, the more than better).\n",
        "\n",
        "## Final Outcome\n",
        "\n",
        "\n",
        "1. Colab Notebook:\n",
        "- Showcases the entire process of web data crawling, including the\n",
        "chosen domains, code implementation, and data extraction.\n",
        "- Clearly comment and document each step in the notebook.\n",
        "2. Dataset Files:\n",
        "- Extracted dataset in a separate file format (e.g., CSV, JSON) that includes a sample of the collected data.\n",
        "3. Summary:\n",
        "- Why specific domains were selected.\n",
        "- Briefly describe the web crawling tools or libraries used and why?\n",
        "- Statistics of data extracted from each domain."
      ],
      "metadata": {
        "id": "EJ7DEKQqJugv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6UKjlnKJkET"
      },
      "outputs": [],
      "source": []
    }
  ]
}