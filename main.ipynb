{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfjZ0unM/CUIRTsQ2aCNM1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theinshort/crawler/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB DATA CRAWLING\n",
        "\n",
        "In this project our aim is to scrap a webiste using python.\n",
        "\n",
        "`This project is for learning purpose and is not intended to perform any kind of violations.`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJ7DEKQqJugv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "We will use a web crawler to acquire data from a specific domain\n",
        "\n",
        "1. Choose a domains of interest (e.g., news articles, product reviews, scientific publications etc).\n",
        "2. Identify and use web crawling tools or libraries (such as BeautifulSoup, Scrapy, or others) to extract data from the chosen domains.\n",
        "3. Collect a sufficient amount of data to ensure diversity and relevance.\n",
        "4. Scrape and clean the HTML contents to generate clean text outputs (at least 2 GB textual data, the more than better)."
      ],
      "metadata": {
        "id": "3eRKIE2cRhTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Outcome\n",
        "\n",
        "\n",
        "1. Colab Notebook:\n",
        "- Showcases the entire process of web data crawling, including the\n",
        "chosen domains, code implementation, and data extraction.\n",
        "- Clearly comment and document each step in the notebook.\n",
        "2. Dataset Files:\n",
        "- Extracted dataset in a separate file format (e.g., CSV, JSON) that includes a sample of the collected data.\n",
        "3. Summary:\n",
        "- Why specific domains were selected.\n",
        "- Briefly describe the web crawling tools or libraries used and why?\n",
        "- Statistics of data extracted from each domain."
      ],
      "metadata": {
        "id": "AJ9JMvRtReM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHOOSING A DOMAIN\n",
        "In this section we will be performing some steps in order to finalize our domain of interest. We will be considering all ethical an legal concerns before staring our scaping process.\n",
        "\n",
        "In order to choose the domain for scrapping, we have to understand the complexity of data and website structure. We will be focusing toward product based websites like shopping stores because data from these website are usually availble to scrap.\n",
        "\n",
        "Data is useful and scrapping from a website without permission is illegal, so before starting to scrap data we need to check if the data available on the website is allowed for scrapping or not.\n",
        "\n",
        "We will be performing following steps to finalize our domains:\n",
        "\n",
        "\n",
        "1.   Decide what type of data we need to scrap.\n",
        "2.   Find related websites\n",
        "3.   Analyze website content and structure\n",
        "4.   Check website robots.txt file to check restrictions\n",
        "5.   Select website if allowed\n",
        "\n"
      ],
      "metadata": {
        "id": "xPA0-anjN0a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decide what type of data we need to scrap.\n",
        "We will be scraping MCQs data, which is usefull in many aspects and also have some dificulties which will help us understand the scrapping procedure in better way.\n",
        "MCQs data has a structure with multiple options, title, answer, explanations, and more. This type of data is usefull in machine learning and fine-tuning models to get desire results."
      ],
      "metadata": {
        "id": "GHHPPnm4RolM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find related websites\n",
        "Some of the websites with the desired data are as follows:\n",
        "1. [PakMCQs ](https://pakmcqs.com/)\n",
        "2. [CSSMCQs](https://cssmcqs.com/)\n",
        "3. [MCQs Forum](https://mcqsforum.com/)\n",
        "4. [MCQs Planet](https://mcqsplanet.com/)\n",
        "5. [Top MCQs](https://topmcqs.com/)\n"
      ],
      "metadata": {
        "id": "KW7c7uztSu3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze website content and structure\n",
        "\n",
        " After analyzing these websites we have colcluded that the structure of website asre different so we can not use a single method for all website, we have to handle each website indivisually."
      ],
      "metadata": {
        "id": "3xetWck-UoDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check website robots.txt file for restrictions\n",
        "\n",
        "In order to scrap a website we have to first check if the website is allowing developers and other users to scrap their content. To check restricions, we need to analyse the website's robots.txt file.\n"
      ],
      "metadata": {
        "id": "Z-QWK9Stb_La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import requests as req\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Creating a function that fetch the content of a robots.txt file\n",
        "def get_robots_txt(url):\n",
        "  \"\"\"Fetch robots.txt file content from given url \"\"\"\n",
        "  file_url = f\"{url}/robots.txt\"\n",
        "  # Fetching data from file using request library get function\n",
        "  response = req.get(file_url)\n",
        "  # Check the status of response before return\n",
        "  if response.status_code == 200:\n",
        "    return response.text\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def check_restrictions(url, robots_txt):\n",
        "  \"\"\"Check the robots.txt rules to check if URL is allowed for scrapping or not \"\"\"\n",
        "\n",
        "  if not robots_txt:\n",
        "    return True\n",
        "\n",
        "  soup = BeautifulSoup(robots_txt, \"html.parser\")\n",
        "  # Adding a user agent header help mimic a real browser and reduce the chances of getting blocked.\n",
        "  user_agents = soup.find_all(\"user-agent\")\n",
        "\n",
        "  for user_agent in user_agents:\n",
        "    # Checks the wildcard for user agent in rorbots.txt file content\n",
        "    if \"*\" in user_agent.text.strip():\n",
        "      for disallow in soup.find_all(\"disallow\"):\n",
        "        disallow_path = disallow.text.strip()\n",
        "        # Check if the url is in the restricted paths or not\n",
        "        if disallow_path in url:\n",
        "          return False\n",
        "  # There are nor restricted rules available in the file. We will consider it as allowed for scrapping\n",
        "  return True\n",
        "\n"
      ],
      "metadata": {
        "id": "zU4EUYU7eoc4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we have done in the above code example is:\n",
        "1. Fetches the requests library for making web requests and BeautifulSoup for parsing HTML content.\n",
        "2. Retrieves the `robots.txt` file from a given URL using `get_robots_txt()` function.\n",
        "3. Analyzes the `robots.txt` content within `check_restrictions()` to determine if a URL is allowed for scraping based on website guidelines before proceeding."
      ],
      "metadata": {
        "id": "L2eRMzPOteBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "site_url = \"https://pakmcqs.com/\"\n",
        "robots_txt = get_robots_txt(site_url)\n",
        "\n",
        "if robots_txt:\n",
        "  # set target url to check for crawling\n",
        "  target_url = \"https://pakmcqs.com/category/english-mcqs\"\n",
        "  allowed = check_restrictions(target_url, robots_txt)\n",
        "  if allowed:\n",
        "    print(f\"Target URL: '{target_url}' is allowed for Crawling\")\n",
        "  else:\n",
        "    print(f\"Target URL: '{target_url}' is not allowed for crawling\")\n",
        "\n",
        "else:\n",
        "  print(\"robots.txt not found, Ask for confirmation of assume this as no restricions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pycl31CQuzqu",
        "outputId": "8e9a8ff1-bc0c-4819-ceba-190142c9bda2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target URL: 'https://pakmcqs.com/category/english-mcqs' is allowed for Crawling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-06d763deb786>:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(robots_txt, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After verification, we can confirm that the target url is allowed for scraping and we can proceed to the next step."
      ],
      "metadata": {
        "id": "NZJv-T0zxHg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_target_urls(base_url, starts_with, html_tag, tag_id):\n",
        "  \"\"\"\n",
        "  Fetch list of target urls from the given base url and match urls string provided with html tag and tag id\n",
        "\n",
        "  Args:\n",
        "      base_url (str): The URL to fetch HTML from.\n",
        "      starts_with (str):The starting URL to compare with the URLs to fetch.\n",
        "      html_tag (str): HTML Tag from which urls needs to be fetched.\n",
        "      tag_id (str): Tag id to fetch target URLs from.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of URLs starting with the starts_with Args found within the html_tag with given tag_id.\n",
        "  \"\"\"\n",
        "\n",
        "  # Send a GET request to the provided URL\n",
        "  response = requests.get(base_url)\n",
        "\n",
        "  # Checkif the URL is active\n",
        "  if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the element with provided html_tag & tag_id\n",
        "    tag_content = soup.find(html_tag, id=tag_id)\n",
        "\n",
        "    # Create an empty list to store the target URLs\n",
        "    target_urls = []\n",
        "\n",
        "    # Check if the div is found\n",
        "    if tag_content:\n",
        "      # Find all anchor tags (a elements) within the div\n",
        "      for anchor in tag_content.find_all('a'):\n",
        "        href = anchor.get('href')\n",
        "        if href and href.startswith(starts_with):\n",
        "          target_urls.append(href)\n",
        "\n",
        "    return target_urls\n",
        "  else:\n",
        "    print(f\"Error fetching URL: {base_url} - Status code: {response.status_code}\")\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "GU0pyaPVDTt5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_category_from_url(url):\n",
        "  url_parts = url.split(\"/\")\n",
        "  return url_parts[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "Srgh235CwsXC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_mcq(url, category):\n",
        "  \"\"\"\n",
        "  Extracts MCQ details (question, answer choices) from a given URL.\n",
        "  Args:\n",
        "      url (str): The URL of the MCQ page.\n",
        "  Returns:\n",
        "      dict: A dictionary containing the extracted MCQ details (question, answers).\n",
        "  \"\"\"\n",
        "  all_mcqs = []\n",
        "\n",
        "  response = requests.get(url)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the header containing the MCQ question\n",
        "    headers = soup.find_all('header', class_='entry-header entry-header-index')\n",
        "    for header in headers:\n",
        "      mcq = {}\n",
        "      if header:\n",
        "        mcq[\"category\"] = category\n",
        "        # Extract question from the anchor tag within the header\n",
        "        question_element = header.find('strong')\n",
        "        if question_element:\n",
        "          mcq['question'] = question_element.text.strip()\n",
        "\n",
        "        # Find the answer choices within the content section\n",
        "        content = header.find('div', class_='entry-content')\n",
        "        if content:\n",
        "          option_elements = content.find_all('p')\n",
        "          # correct_answer = content.find('strong')\n",
        "          if option_elements:\n",
        "            # Assuming the first paragraph contains answer choices (modify if needed)\n",
        "            options = option_elements[0].text.strip().split('\\n')\n",
        "            options_list = [option.strip() for option in options]\n",
        "            for i, option in enumerate(options_list):\n",
        "              mcq[f\"option {i+1}\"] = re.sub(r\"(A\\. |B\\. |C\\. |D\\. |E\\. )\", \"\", option)\n",
        "\n",
        "          # Identify bold answer by searching for strong tags within paragraphs\n",
        "          bold_answer = \"\"\n",
        "          for option_element in option_elements:\n",
        "            strong_element = option_element.find('strong')\n",
        "            if strong_element:\n",
        "              bold_answer = strong_element.text.strip()\n",
        "              break  # Exit after finding the first bold element (assuming only one)\n",
        "          if bold_answer:\n",
        "            mcq['correct_answer'] = bold_answer.strip()[0]\n",
        "            all_mcqs.append(mcq)\n",
        "\n",
        "  return all_mcqs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pOtYexCKzVnY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_page_number(url):\n",
        "  max_page = 1000\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    page_nav = soup.find(\"div\", class_=\"wpsp-page-nav\")\n",
        "    page_numbers = page_nav.find_all(\"a\")\n",
        "    max_page = int(page_numbers[-2].text)\n",
        "\n",
        "  return max_page\n"
      ],
      "metadata": {
        "id": "eooImPuyd0iq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_page_mcqs(url, last_page):\n",
        "  all_mcqs = []\n",
        "  category = get_category_from_url(url)\n",
        "  for x in range(1,last_page+1):\n",
        "    mcqs = extract_mcq(url+f\"/page/{x}\", category)\n",
        "    all_mcqs.extend(mcqs)\n",
        "\n",
        "  write_mcqs_to_csv(all_mcqs, \"mcqs.csv\")\n",
        "  return all_mcqs\n"
      ],
      "metadata": {
        "id": "-IV2FCekcc4j"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading as td\n",
        "\n",
        "def fetch_all_mcqs(urls):\n",
        "  threads = []\n",
        "\n",
        "  def process_url(url):\n",
        "    max_page = get_max_page_number(url)\n",
        "    mcqs = get_all_page_mcqs(url, max_page)\n",
        "\n",
        "  for url in urls:\n",
        "    thread = td.Thread(target=process_url, args=(url,))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "  for thread in threads:\n",
        "    thread.join()\n",
        "\n"
      ],
      "metadata": {
        "id": "7HjLHtrTbWPj"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def write_mcqs_to_csv(all_mcqs, file_name):\n",
        "  with open(file_name, \"a\", newline='') as csv_file:\n",
        "    columns = [\"question\", \"option 1\", \"option 2\", \"option 3\", \"option 4\",\"option 5\", \"correct_answer\", \"category\"]\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=columns)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_mcqs)"
      ],
      "metadata": {
        "id": "VLmKU08riyDq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_url = \"https://pakmcqs.com/\"  # Website url to fect the categories urls\n",
        "\n",
        "target_urls = get_target_urls(base_url,'https://pakmcqs.com/category/', 'div', 'secondary')\n",
        "\n",
        "if target_urls:\n",
        "  print(f\"Extracted category URLs: {target_urls}\")\n",
        "else:\n",
        "  print(\"No category URLs found in the provided URL or error fetching the content.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PCfl9pihJEJ",
        "outputId": "61a13518-4a8a-486a-ef29-53c60c154c32"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted category URLs: ['https://pakmcqs.com/category/english-mcqs', 'https://pakmcqs.com/category/mathematics-mcqs', 'https://pakmcqs.com/category/general_knowledge_mcqs', 'https://pakmcqs.com/category/pakistan-current-affairs-mcqs', 'https://pakmcqs.com/category/world-current-affairs-mcqs', 'https://pakmcqs.com/category/pak-study-mcqs', 'https://pakmcqs.com/category/islamic-studies-mcqs', 'https://pakmcqs.com/category/computer-mcqs', 'https://pakmcqs.com/category/everyday-science-mcqs', 'https://pakmcqs.com/category/physics-mcqs', 'https://pakmcqs.com/category/chemistry-mcqs', 'https://pakmcqs.com/category/biology-mcqs', 'https://pakmcqs.com/category/pedagogy-mcqs', 'https://pakmcqs.com/category/urdu-general-knowledge', 'https://pakmcqs.com/category/finance-mcqs', 'https://pakmcqs.com/category/hrm-mcqs', 'https://pakmcqs.com/category/marketing-mcqs', 'https://pakmcqs.com/category/accounting-mcqs', 'https://pakmcqs.com/category/auditing-mcqs', 'https://pakmcqs.com/category/electrical-engineering-mcqs', 'https://pakmcqs.com/category/civil-engineering-mcqs', 'https://pakmcqs.com/category/mechanical-engineering-mcqs', 'https://pakmcqs.com/category/chemical-engineering', 'https://pakmcqs.com/category/software-engineering-mcqs', 'https://pakmcqs.com/category/medical-mcqs', 'https://pakmcqs.com/category/medical-mcqs/biochemistry', 'https://pakmcqs.com/category/medical-mcqs/dental-materials', 'https://pakmcqs.com/category/medical-mcqs/general-anatomy-mcqs', 'https://pakmcqs.com/category/medical-mcqs/microbiology', 'https://pakmcqs.com/category/medical-mcqs/oral-anatomy', 'https://pakmcqs.com/category/medical-mcqs/oral-histology', 'https://pakmcqs.com/category/medical-mcqs/oral-pathology-and-medicine', 'https://pakmcqs.com/category/medical-mcqs/physiology-mcqs', 'https://pakmcqs.com/category/medical-mcqs/pathology', 'https://pakmcqs.com/category/medical-mcqs/pharmacology', 'https://pakmcqs.com/category/psychology-mcqs', 'https://pakmcqs.com/category/agriculture-mcqs', 'https://pakmcqs.com/category/economics-mcqs', 'https://pakmcqs.com/category/sociology-mcqs', 'https://pakmcqs.com/category/political-science-mcqs', 'https://pakmcqs.com/category/statistics-mcqs', 'https://pakmcqs.com/category/english-literature-mcqs', 'https://pakmcqs.com/category/judiciary-and-law-mcqs', 'https://pakmcqs.com/category/election-officer-mcqs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_mcqs = fetch_all_mcqs(target_urls)"
      ],
      "metadata": {
        "id": "ai165NPGbUkr"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}